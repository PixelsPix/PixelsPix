{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "15bfe803",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import float64\n",
    "from numpy.typing import NDArray, ArrayLike\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sci\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow import Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rede_Neural:\n",
    "    def __init__(self, qtd_neuronios_camada, seed) -> None:\n",
    "        self.camadas: int = len(qtd_neuronios_camada)\n",
    "        self.neuronios_camada: list[int] = qtd_neuronios_camada\n",
    "        self.pesos = [self.pesos_iniciais(seed)]\n",
    "        self.seed = seed\n",
    "\n",
    "    def ativacao(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Aplica a função de ativação definida em uma lista ou np.array. Atualmente é tanh, pq ReLU e Leaku ReLU é linear demais pro que queremos.\n",
    "        \"\"\"\n",
    "        return 5 * tf.tanh(tf.divide(z, 5))\n",
    "\n",
    "    def derivada_ativacao(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Aplica a derivada da função de ativação definida em uma lista ou np.array. Atualmente é leaky ReLU, com 0.01 se z < 0\n",
    "        \"\"\"\n",
    "        return 1 / tf.cosh(tf.divide(z,5)) ** 2\n",
    "\n",
    "    def pesos_iniciais(self, seed: int) -> list[Tensor]:\n",
    "        \"\"\"\n",
    "        Usa a distribuição de Xavier para criar pesos iniciais usando a seed dada para o rng.\\n\n",
    "        W(camada) ~ N(0, sqrt(2 / neuronios[camada]))\n",
    "        \"\"\"\n",
    "        pesos_saida = []\n",
    "        tf.random.set_seed(seed)\n",
    "        \n",
    "        for i in range(self.camadas-1):\n",
    "            n_in = self.neuronios_camada[i]\n",
    "            n_out = self.neuronios_camada[i+1]\n",
    "\n",
    "            w = tf.Variable(tf.keras.initializers.GlorotNormal(seed=seed)((n_out, n_in + 1)))\n",
    "\n",
    "            pesos_saida.append(w)\n",
    "\n",
    "        return pesos_saida\n",
    "\n",
    "    def forward_pass(self, entrada: ArrayLike, pesos: list[Tensor]) -> list[dict[str, Tensor]]:\n",
    "        \"\"\"\n",
    "        Aplica a rotina de multiplicar pelos pesos e aplicar função de ativação para todas as camadas, retornando os valores pré-ativação e pós-ativação de cada camada.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        entrada: ArrayLike\n",
    "            É a lista de valores de entrada da rede, pode ser uma lista mesmo ou qualquer ArrayLike, já que é transformada em Tensor dentro da função.\n",
    "\n",
    "        pesos: list[Tensor]\n",
    "            São os pesos que se quer usar na rede. Usar self.pesos[-1] para última época\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        valores_camadas: list[dict]\n",
    "            Para uma camada qualquer ``i``, os valores de ``dicionário valores_camadas[i]`` são:\n",
    "\n",
    "            - ``\"z\"``: Tensor or None\\n\n",
    "                Valores não ativados da camada\n",
    "            - ``\"a\"``: Tensor\\n\n",
    "                Valores ativados da camada\n",
    "        \"\"\"\n",
    "        valores_camadas = []\n",
    "        a = tf.convert_to_tensor(entrada, dtype=tf.float32)\n",
    "        valores_camadas.append({\"z\": None, \"a\": a})\n",
    "\n",
    "        for W in pesos:\n",
    "            a_bias = tf.concat([a, tf.ones([1])], axis=0) # adiciona bias\n",
    "            z = tf.linalg.matvec(W, a_bias)\n",
    "            a = self.ativacao(z)\n",
    "            valores_camadas.append({\"z\": z, \"a\": a})\n",
    "        \n",
    "        return valores_camadas\n",
    "    \n",
    "    def previsao(self, entrada: ArrayLike, epoca: int) -> NDArray:\n",
    "        \"\"\"Retorna os valores previstos pela rede, os valores ativados da última camada, para a época desejada. Usar -1 para última época\"\"\"\n",
    "        camadas  = self.forward_pass(entrada, self.pesos[epoca])\n",
    "        previsao = camadas[-1][\"a\"]\n",
    "        return previsao.numpy()\n",
    "\n",
    "    def perda(self, previsao: Tensor, real: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Calcula a função de perda da rede, atualmente usando 0.5 MSE mas poderia ser com penalidades e\n",
    "        para função de valor máximo talvez seja melhor usar uma função como softmax\n",
    "        \"\"\"\n",
    "        residuos = previsao - real\n",
    "        mse = tf.reduce_mean(residuos ** 2)\n",
    "        perda = 0.5 * mse\n",
    "        return perda\n",
    "    \n",
    "    # def derivada_perda(self, previsao: Tensor, real: Tensor) -> Tensor:\n",
    "    #     \"\"\"\n",
    "    #     Calcula a derivada da função de perda, lembrar que a derivada de abs(x) = sign(x)\n",
    "    #     \"\"\"\n",
    "    #     residuos = previsao - real\n",
    "    #     return residuos\n",
    "    \n",
    "    def calcular_gradiente(self, entrada: Tensor, real: Tensor, epoca: int) -> list[Tensor]:\n",
    "        \"\"\"Calcula o gradiente dos pesos pelo tensorflow, retornando uma lista de Tensores correspondente ao gradiente de cada camada.\"\"\"\n",
    "        pesos = self.pesos[epoca]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            previsao = self.forward_pass(entrada, pesos)[-1][\"a\"]\n",
    "            perda = self.perda(previsao, real)\n",
    "\n",
    "        gradiente = tape.gradient(perda, pesos)\n",
    "        return gradiente\n",
    "\n",
    "    def treinar_uma_epoca(self, entradas: list[Tensor], saidas: list[Tensor], tamanho_lote: int, taxa_aprendizado: float) -> float:\n",
    "        \"\"\"Treina a rede por uma época usando Adam e mini-batches\"\"\"\n",
    "        # embaralha\n",
    "        n = len(entradas)\n",
    "        indices = np.arange(n)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        # pega os pesos atuais e inicializa Adam\n",
    "        pesos = self.pesos[-1]\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=taxa_aprendizado)\n",
    "\n",
    "        perdas = []\n",
    "\n",
    "        for inicio in range(0, n, tamanho_lote):\n",
    "            fim = inicio + tamanho_lote\n",
    "            batch_idx = indices[inicio:fim]\n",
    "\n",
    "            # monta o batch\n",
    "            x_batch = tf.stack([entradas[i] for i in batch_idx])\n",
    "            y_batch = tf.stack([saidas[i] for i in batch_idx])\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                # forward em todo o batch\n",
    "                previsoes = [self.forward_pass(x, pesos)[-1][\"a\"] for x in x_batch]\n",
    "                previsoes = tf.stack(previsoes)\n",
    "\n",
    "                perda = self.perda(previsoes, y_batch)\n",
    "\n",
    "            # gradiente médio do batch\n",
    "            gradientes = tape.gradient(perda, pesos)\n",
    "            optimizer.apply_gradients(zip(gradientes, pesos))\n",
    "\n",
    "            perdas.append(perda.numpy())\n",
    "\n",
    "        # salva pesos da época\n",
    "        self.pesos.append([tf.identity(w) for w in pesos])\n",
    "\n",
    "        return float(np.mean(perdas))\n",
    "\n",
    "    def treinar(self, taxa_aprendizado: float, tamanho_lotes: int, entradas_treino: list[ArrayLike], saidas_treino: list[ArrayLike], quantidade_epocas: int) -> None:\n",
    "        \"\"\"Chama self.treinar_uma_epoca ``quantidade_epocas`` vezes\"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        for epoca in range(quantidade_epocas):\n",
    "            self.treinar_uma_epoca(taxa_aprendizado, tamanho_lotes, entradas_treino, saidas_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd43dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "re"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
