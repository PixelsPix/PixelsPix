{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be395a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import float64\n",
    "from numpy.typing import NDArray, ArrayLike\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rede_Neural:\n",
    "    def __init__(self, qtd_neuronios_camada, seed) -> None:\n",
    "        self.camadas: int = len(qtd_neuronios_camada)\n",
    "        self.neuronios_camada: list[int] = qtd_neuronios_camada\n",
    "        self.pesos = [self.pesos_iniciais(seed)]\n",
    "        self.seed = seed\n",
    "\n",
    "    def ativacao(self, z: ArrayLike) -> NDArray:\n",
    "        \"\"\"\n",
    "        Aplica a função de ativação definida em uma lista ou np.array. Atualmente é leaky ReLU, com 0.01*z se z < 0\n",
    "        \"\"\"\n",
    "        return np.where(z > 0, z, 0.01 * z)\n",
    "\n",
    "    def derivada_ativacao(self, z: ArrayLike) -> NDArray:\n",
    "        \"\"\"\n",
    "        Aplica a derivada da função de ativação definida em uma lista ou np.array. Atualmente é leaky ReLU, com 0.01 se z < 0\n",
    "        \"\"\"\n",
    "        return np.where(z > 0, 1, 0.01)\n",
    "\n",
    "    def pesos_iniciais(self, seed: int) -> list[NDArray[float64]]:\n",
    "        \"\"\"\n",
    "        Usa a distribuição de Kaiming para criar pesos iniciais para ReLU usando a seed dada para o rng.\\n\n",
    "        W(camada) ~ N(0, sqrt(2 / neuronios[camada]))\n",
    "        \"\"\"\n",
    "        pesos_lista = []\n",
    "        np.random.seed(seed)\n",
    "        for i in range(self.camadas - 1):\n",
    "            pesos = np.random.normal(\n",
    "                0,\n",
    "                np.sqrt(2/self.neuronios_camada[i]),\n",
    "                size=(self.neuronios_camada[i+1], self.neuronios_camada[i]+1)\n",
    "            )\n",
    "            pesos_lista.append(pesos)\n",
    "        return pesos_lista\n",
    "\n",
    "    def forward_pass(self, entrada: ArrayLike, pesos: list[NDArray]) -> list[dict[str, NDArray]]:\n",
    "        \"\"\"\n",
    "        Aplica a rotina de multiplicar pelos pesos e aplicar função de ativação para todas as camadas, retornando os valores pré-ativação e pós-ativação de cada camada.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        entrada: ArrayLike\n",
    "            É a lista de valores de entrada da rede, pode ser uma lista mesmo ou qualquer ArrayLike, já que é transformada em NDArray dentro da função.\n",
    "\n",
    "        pesos: list[NDArray]\n",
    "            São os pesos que se quer usar na rede. Usar self.pesos[-1] para última época\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        valores_camadas: list[dict]\n",
    "            Para uma camada qualquer ``i``, os valores de ``dicionário valores_camadas[i]`` são:\n",
    "\n",
    "            - ``\"z\"``: NDArray or None\\n\n",
    "                Valores não ativados da camada\n",
    "            - ``\"a\"``: NDArray\\n\n",
    "                Valores ativados da camada\n",
    "        \"\"\"\n",
    "        valores_camadas = []\n",
    "        a = np.array(entrada)\n",
    "        valores_camadas.append({\"z\": None, \"a\": a})\n",
    "\n",
    "        for W in pesos:\n",
    "            z = W @ np.append(a, 1) # adiciona bias\n",
    "            a = self.ativacao(z)\n",
    "            valores_camadas.append({\"z\": z, \"a\": a})\n",
    "        \n",
    "        return valores_camadas\n",
    "    \n",
    "    def previsao(self, entrada: ArrayLike, epoca: int) -> NDArray:\n",
    "        \"\"\"Retorna os valores previstos pela rede, os valores ativados da última camada, para a época desejada. Usar -1 para última época\"\"\"\n",
    "        camadas  = self.forward_pass(entrada, self.pesos[epoca])\n",
    "        previsao = camadas[-1][\"a\"]\n",
    "        return previsao\n",
    "\n",
    "    def perda(self, previsao: NDArray, real: ArrayLike) -> float64:\n",
    "        \"\"\"\n",
    "        Calcula a função de perda da rede, atualmente usando 0.5 MSE mas poderia ser com penalidades e\n",
    "        para função de valor máximo talvez seja melhor usar uma função como softmax\n",
    "        \"\"\"\n",
    "        residuos: NDArray = previsao - real\n",
    "        mse = np.mean(residuos ** 2)\n",
    "        perda = 0.5 * mse\n",
    "        return perda\n",
    "    \n",
    "    def derivada_perda(self, previsao: NDArray, real: ArrayLike) -> NDArray:\n",
    "        \"\"\"\n",
    "        Calcula a derivada da função de perda, lembrar que a derivada de abs(x) = sign(x)\n",
    "        \"\"\"\n",
    "        residuos: NDArray = previsao - real\n",
    "        return residuos\n",
    "    \n",
    "    def calcular_gradiente(self, entrada: ArrayLike, real: ArrayLike, pesos: list[NDArray]) -> list[NDArray]:\n",
    "        \"\"\"Calcula o gradiente dos pesos para os pesos dados, retornando uma lista de NDArrays correspondente ao gradiente de cada camada.\"\"\"\n",
    "        valores_camadas = self.forward_pass(entrada, pesos)\n",
    "        previsao = valores_camadas[-1][\"a\"]\n",
    "        \n",
    "        # alocacao de listas\n",
    "        delta = [None] * (self.camadas - 1)\n",
    "        gradiente = [None] * (self.camadas - 1)\n",
    "    \n",
    "        # delta da ultima camada\n",
    "        erro_saida = self.derivada_perda(previsao, real)\n",
    "        delta[-1] = erro_saida * self.derivada_ativacao(valores_camadas[-1][\"z\"])\n",
    "\n",
    "        # gradiente da ultima camada\n",
    "        a_anterior = np.append(valores_camadas[-2][\"a\"], 1)\n",
    "        gradiente[-1] = np.outer(delta[-1], a_anterior)\n",
    "\n",
    "        # gradiente das camadas ocultas\n",
    "        for camada in reversed(range(self.camadas - 2)):\n",
    "            # delta da camada\n",
    "            W_sem_bias = pesos[camada + 1][:, :-1] # descarta coluna do bias\n",
    "            delta[camada] = (W_sem_bias.T @ delta[camada + 1]) * self.derivada_ativacao(valores_camadas[camada + 1][\"z\"])\n",
    "            \n",
    "            # gradiente da camada\n",
    "            a_anterior = np.append(valores_camadas[camada][\"a\"], 1)\n",
    "            gradiente[camada] = np.outer(delta[camada], a_anterior)\n",
    "\n",
    "        return gradiente\n",
    "\n",
    "    def treinar_uma_epoca(self, taxa_aprendizado: float, tamanho_lotes: int, entradas_treino: list[ArrayLike], saidas_treino: list[ArrayLike]) -> None:\n",
    "        \"\"\"Calcula os gradientes e corrige os pesos de acordo com os parâmetros dados, salvando os novos pesos em self.pesos.\"\"\"\n",
    "        # embaralha os indices\n",
    "        n = len(entradas_treino)\n",
    "        indices = np.arange(n)\n",
    "        np.random.shuffle(indices)\n",
    "        novos_pesos = [W.copy() for W in self.pesos[-1]] # usa a ultima epoca\n",
    "\n",
    "        # percorre os lotes\n",
    "        for inicio in range(0, n, tamanho_lotes):\n",
    "            fim = inicio + tamanho_lotes\n",
    "            indices_lote = indices[inicio:fim]\n",
    "            gradiente_acumulado = [np.zeros_like(W) for W in novos_pesos]\n",
    "\n",
    "            # calcula o gradiente e soma no gradiente acumulado\n",
    "            for i in indices_lote:\n",
    "                entrada = np.array(entradas_treino[i])\n",
    "                saida_real = np.array(saidas_treino[i])\n",
    "\n",
    "                gradiente = self.calcular_gradiente(entrada, saida_real, novos_pesos)\n",
    "\n",
    "                for j in range(self.camadas - 1):\n",
    "                    gradiente_acumulado[j] += gradiente[j]\n",
    "\n",
    "            # tira o gradiente medio\n",
    "            gradiente_medio = [g / len(indices_lote) for g in gradiente_acumulado]\n",
    "            \n",
    "            # corrige os pesos\n",
    "            for i in range(self.camadas-1):\n",
    "                novos_pesos[i] -= taxa_aprendizado * gradiente_medio[i]\n",
    "        \n",
    "        self.pesos.append(novos_pesos)\n",
    "\n",
    "    def treinar(self, taxa_aprendizado: float, tamanho_lotes: int, entradas_treino: list[ArrayLike], saidas_treino: list[ArrayLike], quantidade_epocas: int) -> None:\n",
    "        \"\"\"Chama self.treinar_uma_epoca ``quantidade_epocas`` vezes\"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        for epoca in range(quantidade_epocas):\n",
    "            self.treinar_uma_epoca(taxa_aprendizado, tamanho_lotes, entradas_treino, saidas_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89be4a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "rede = Rede_Neural([2,3,2], 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3fcec26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00734121, -0.04552614])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede.previsao([1,2],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3312e76",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
