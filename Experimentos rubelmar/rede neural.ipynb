{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be395a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import float64\n",
    "from numpy.typing import NDArray, ArrayLike\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy as sci\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265aec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rede_Neural:\n",
    "    def __init__(self, qtd_neuronios_camada, seed) -> None:\n",
    "        self.camadas: int = len(qtd_neuronios_camada)\n",
    "        self.neuronios_camada: list[int] = qtd_neuronios_camada\n",
    "        self.pesos = [self.pesos_iniciais(seed)]\n",
    "        self.seed = seed\n",
    "        self.perdas = []\n",
    "\n",
    "    def ativacao(self, z: ArrayLike) -> NDArray:\n",
    "        \"\"\"\n",
    "        Aplica a função de ativação definida em uma lista ou np.array. Atualmente é tanh, pq ReLU e Leaku ReLU é linear demais pro que queremos.\n",
    "        \"\"\"\n",
    "        return 5 * np.tanh(z/5)\n",
    "\n",
    "    def derivada_ativacao(self, z: ArrayLike) -> NDArray:\n",
    "        \"\"\"\n",
    "        Aplica a derivada da função de ativação definida em uma lista ou np.array. Atualmente é leaky ReLU, com 0.01 se z < 0\n",
    "        \"\"\"\n",
    "        return 1 / np.cosh(z/5) ** 2\n",
    "\n",
    "    def pesos_iniciais(self, seed: int) -> list[NDArray[float64]]:\n",
    "        \"\"\"\n",
    "        Usa a distribuição de Xavier para criar pesos iniciais usando a seed dada para o rng.\\n\n",
    "        W(camada) ~ N(0, sqrt(2 / neuronios[camada]))\n",
    "        \"\"\"\n",
    "        pesos_saida = []\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Inicializacao de Kaiming\n",
    "        # for i in range(self.camadas - 1):\n",
    "        #     n_in = self.neuronios_camada[i]\n",
    "        #     n_out = self.neuronios_camada[i+1]\n",
    "        #     pesos = np.random.normal(\n",
    "        #         0,\n",
    "        #         np.sqrt(2/n_in),\n",
    "        #         size=(n_out, n_in+1)\n",
    "        #     )\n",
    "        #     pesos_saida.append(pesos)\n",
    "\n",
    "        # Inicializacao de Xavier\n",
    "        for i in range(self.camadas - 1):\n",
    "            n_in = self.neuronios_camada[i]\n",
    "            n_out = self.neuronios_camada[i+1]\n",
    "            pesos = np.random.normal(\n",
    "                0,\n",
    "                np.sqrt(2 / (n_in + n_out)),\n",
    "                size=(n_out, n_in+1)\n",
    "            )\n",
    "            pesos_saida.append(pesos)\n",
    "\n",
    "        return pesos_saida\n",
    "\n",
    "    def forward_pass(self, entrada: ArrayLike, pesos: list[NDArray]) -> list[dict[str, NDArray]]:\n",
    "        \"\"\"\n",
    "        Aplica a rotina de multiplicar pelos pesos e aplicar função de ativação para todas as camadas, retornando os valores pré-ativação e pós-ativação de cada camada.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        entrada: ArrayLike\n",
    "            É a lista de valores de entrada da rede, pode ser uma lista mesmo ou qualquer ArrayLike, já que é transformada em NDArray dentro da função.\n",
    "\n",
    "        pesos: list[NDArray]\n",
    "            São os pesos que se quer usar na rede. Usar self.pesos[-1] para última época\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        valores_camadas: list[dict]\n",
    "            Para uma camada qualquer ``i``, os valores de ``dicionário valores_camadas[i]`` são:\n",
    "\n",
    "            - ``\"z\"``: NDArray or None\\n\n",
    "                Valores não ativados da camada\n",
    "            - ``\"a\"``: NDArray\\n\n",
    "                Valores ativados da camada\n",
    "        \"\"\"\n",
    "        valores_camadas = []\n",
    "        a = np.array(entrada)\n",
    "        valores_camadas.append({\"z\": None, \"a\": a})\n",
    "\n",
    "        for W in pesos:\n",
    "            z = W @ np.append(a, 1) # adiciona bias\n",
    "            a = self.ativacao(z)\n",
    "            valores_camadas.append({\"z\": z, \"a\": a})\n",
    "        \n",
    "        return valores_camadas\n",
    "    \n",
    "    def previsao(self, entrada: ArrayLike, epoca: int) -> NDArray:\n",
    "        \"\"\"Retorna os valores previstos pela rede, os valores ativados da última camada, para a época desejada. Usar -1 para última época\"\"\"\n",
    "        camadas  = self.forward_pass(entrada, self.pesos[epoca])\n",
    "        previsao = camadas[-1][\"a\"]\n",
    "        return previsao\n",
    "\n",
    "    def perda(self, previsao: NDArray, real: ArrayLike) -> float:\n",
    "        \"\"\"\n",
    "        Calcula a função de perda da rede, atualmente usando 0.5 MSE mas poderia ser com penalidades e\n",
    "        para função de valor máximo talvez seja melhor usar uma função como softmax\n",
    "        \"\"\"\n",
    "        residuos: NDArray = previsao - real\n",
    "        mse = np.mean(residuos ** 2)\n",
    "        perda = 0.5 * mse\n",
    "        return perda\n",
    "    \n",
    "    def derivada_perda(self, previsao: NDArray, real: ArrayLike) -> NDArray:\n",
    "        \"\"\"\n",
    "        Calcula a derivada da função de perda, lembrar que a derivada de abs(x) = sign(x)\n",
    "        \"\"\"\n",
    "        residuos: NDArray = previsao - real\n",
    "        return residuos\n",
    "    \n",
    "    def calcular_gradiente(self, entrada: ArrayLike, real: ArrayLike, pesos: list[NDArray]) -> list[NDArray]:\n",
    "        \"\"\"Calcula o gradiente dos pesos para os pesos dados, retornando uma lista de NDArrays correspondente ao gradiente de cada camada.\"\"\"\n",
    "        valores_camadas = self.forward_pass(entrada, pesos)\n",
    "        previsao = valores_camadas[-1][\"a\"]\n",
    "        \n",
    "        # alocacao de listas\n",
    "        delta = [None] * (self.camadas - 1)\n",
    "        gradiente = [None] * (self.camadas - 1)\n",
    "    \n",
    "        # delta da ultima camada\n",
    "        erro_saida = self.derivada_perda(previsao, real)\n",
    "        delta[-1] = erro_saida * self.derivada_ativacao(valores_camadas[-1][\"z\"])\n",
    "\n",
    "        # gradiente da ultima camada\n",
    "        a_anterior = np.append(valores_camadas[-2][\"a\"], 1)\n",
    "        gradiente[-1] = np.outer(delta[-1], a_anterior)\n",
    "\n",
    "        # gradiente das camadas ocultas\n",
    "        for camada in reversed(range(self.camadas - 2)):\n",
    "            # delta da camada\n",
    "            W_sem_bias = pesos[camada + 1][:, :-1] # descarta coluna do bias\n",
    "            delta[camada] = (W_sem_bias.T @ delta[camada + 1]) * self.derivada_ativacao(valores_camadas[camada + 1][\"z\"])\n",
    "            \n",
    "            # gradiente da camada\n",
    "            a_anterior = np.append(valores_camadas[camada][\"a\"], 1)\n",
    "            gradiente[camada] = np.outer(delta[camada], a_anterior)\n",
    "\n",
    "        return gradiente\n",
    "\n",
    "    def treinar_uma_epoca(self, taxa_aprendizado: float, tamanho_lotes: int, entradas_treino: list[ArrayLike], saidas_treino: list[ArrayLike]) -> None:\n",
    "        \"\"\"Calcula os gradientes e corrige os pesos de acordo com os parâmetros dados, salvando os novos pesos em self.pesos. Usando Adam do TensorFlow\"\"\"\n",
    "        # embaralha os indices\n",
    "        n = len(entradas_treino)\n",
    "        indices = np.arange(n)\n",
    "        np.random.shuffle(indices)\n",
    "        perdas_lote = []\n",
    "\n",
    "        # pesos para atualizar e inicializa adam\n",
    "        tf_pesos = [tf.Variable(W, dtype=tf.float32) for W in self.pesos[-1]] # usa a ultima epoca\n",
    "        adam = tf.keras.optimizers.Adam(learning_rate=taxa_aprendizado)\n",
    "\n",
    "        # percorre os lotes\n",
    "        for inicio in range(0, n, tamanho_lotes):\n",
    "            fim = inicio + tamanho_lotes\n",
    "            indices_lote = indices[inicio:fim]\n",
    "\n",
    "            # inicializacao dos pesos e gradientes\n",
    "            pesos_numpy = [w.numpy() for w in tf_pesos]\n",
    "            gradiente_acumulado = [np.zeros_like(W) for W in pesos_numpy]\n",
    "\n",
    "            # calcula o gradiente e soma no gradiente acumulado\n",
    "            for i in indices_lote:\n",
    "                entrada = np.array(entradas_treino[i], dtype=np.float32)\n",
    "                saida_real = np.array(saidas_treino[i], dtype=np.float32)\n",
    "\n",
    "                gradiente = self.calcular_gradiente(entrada, saida_real, pesos_numpy)\n",
    "                previsao = self.previsao(entrada, -1)\n",
    "                perdas_lote.append(self.perda(previsao, saida_real))\n",
    "\n",
    "                for j in range(self.camadas - 1):\n",
    "                    gradiente_acumulado[j] += gradiente[j]\n",
    "\n",
    "            # tira o gradiente medio\n",
    "            gradiente_medio = [g / len(indices_lote) for g in gradiente_acumulado]\n",
    "            \n",
    "            # converte gradiente pra tf.tensor e aplica Adam\n",
    "            tf_gradientes = [tf.convert_to_tensor(g, dtype=tf.float32) for g in gradiente_medio]\n",
    "            adam.apply_gradients(zip(tf_gradientes, tf_pesos))\n",
    "        \n",
    "        # sumario da epoca\n",
    "        perda_epoca = np.mean(perdas_lote)\n",
    "        pesos_numpy = [w.numpy() for w in tf_pesos]\n",
    "        \n",
    "        self.pesos.append(pesos_numpy)\n",
    "        self.perdas.append(perda_epoca)\n",
    "\n",
    "    def treinar(self, taxa_aprendizado: float, tamanho_lotes: int, entradas_treino: list[ArrayLike], saidas_treino: list[ArrayLike], quantidade_epocas: int) -> None:\n",
    "        \"\"\"Chama self.treinar_uma_epoca ``quantidade_epocas`` vezes\"\"\"\n",
    "        np.random.seed(self.seed)\n",
    "        for epoca in range(quantidade_epocas):\n",
    "            self.treinar_uma_epoca(taxa_aprendizado, tamanho_lotes, entradas_treino, saidas_treino)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89be4a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "rede = Rede_Neural([2,3,2], 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a3fcec26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.28570615, -2.26300906])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rede.previsao([1,2],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a3312e76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sample</th>\n",
       "      <th>Pot (kW)</th>\n",
       "      <th>Speed (mm/s)</th>\n",
       "      <th>Thickness (mm)</th>\n",
       "      <th>Theta (°)</th>\n",
       "      <th>Radius (mm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2T-01</td>\n",
       "      <td>4.053</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0</td>\n",
       "      <td>4.211640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2T-01</td>\n",
       "      <td>4.053</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.3</td>\n",
       "      <td>1</td>\n",
       "      <td>4.150367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>2T-01</td>\n",
       "      <td>4.053</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.3</td>\n",
       "      <td>2</td>\n",
       "      <td>4.029631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2T-01</td>\n",
       "      <td>4.053</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.791549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>2T-01</td>\n",
       "      <td>4.053</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.3</td>\n",
       "      <td>4</td>\n",
       "      <td>3.721391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8544</th>\n",
       "      <td>2T-01</td>\n",
       "      <td>4.053</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.3</td>\n",
       "      <td>356</td>\n",
       "      <td>3.130822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8568</th>\n",
       "      <td>2T-01</td>\n",
       "      <td>4.053</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.3</td>\n",
       "      <td>357</td>\n",
       "      <td>3.319888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8592</th>\n",
       "      <td>2T-01</td>\n",
       "      <td>4.053</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.3</td>\n",
       "      <td>358</td>\n",
       "      <td>3.567133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8616</th>\n",
       "      <td>2T-01</td>\n",
       "      <td>4.053</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.3</td>\n",
       "      <td>359</td>\n",
       "      <td>4.083982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8640</th>\n",
       "      <td>2T-01</td>\n",
       "      <td>4.053</td>\n",
       "      <td>6.67</td>\n",
       "      <td>6.3</td>\n",
       "      <td>360</td>\n",
       "      <td>4.211640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>361 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sample  Pot (kW)  Speed (mm/s)  Thickness (mm) Theta (°)  Radius (mm)\n",
       "0     2T-01     4.053          6.67             6.3         0     4.211640\n",
       "24    2T-01     4.053          6.67             6.3         1     4.150367\n",
       "48    2T-01     4.053          6.67             6.3         2     4.029631\n",
       "72    2T-01     4.053          6.67             6.3         3     3.791549\n",
       "96    2T-01     4.053          6.67             6.3         4     3.721391\n",
       "...     ...       ...           ...             ...       ...          ...\n",
       "8544  2T-01     4.053          6.67             6.3       356     3.130822\n",
       "8568  2T-01     4.053          6.67             6.3       357     3.319888\n",
       "8592  2T-01     4.053          6.67             6.3       358     3.567133\n",
       "8616  2T-01     4.053          6.67             6.3       359     4.083982\n",
       "8640  2T-01     4.053          6.67             6.3       360     4.211640\n",
       "\n",
       "[361 rows x 6 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"2018 Cruz Neto Polar wide.xlsx\")\n",
    "df = df.drop(columns=df.columns[0])\n",
    "df = df.melt(id_vars=[\"Sample\", \"Pot (kW)\", \"Speed (mm/s)\", \"Thickness (mm)\"],\n",
    "             var_name=\"Theta (°)\",\n",
    "             value_name=\"Radius (mm)\")\n",
    "\n",
    "grupos = df.groupby(\"Sample\")\n",
    "grupos.get_group(\"2T-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5237ddf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pot (kW)         -1.921290\n",
      "Speed (mm/s)     -1.300740\n",
      "Thickness (mm)   -1.364248\n",
      "Theta (°)        -1.727260\n",
      "Radius (mm)      -1.614410\n",
      "dtype: float64 \n",
      "\n",
      "Pot (kW)          1.364187\n",
      "Speed (mm/s)      1.421644\n",
      "Thickness (mm)    1.376163\n",
      "Theta (°)         1.727260\n",
      "Radius (mm)       3.977823\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_normalizado = df.copy()\n",
    "df_normalizado[[\"Pot (kW)\", \"Speed (mm/s)\", \"Thickness (mm)\", \"Theta (°)\", \"Radius (mm)\"]] = StandardScaler().fit_transform(df_normalizado[[\"Pot (kW)\", \"Speed (mm/s)\", \"Thickness (mm)\", \"Theta (°)\", \"Radius (mm)\"]])\n",
    "print(df_normalizado.drop(columns=\"Sample\").min(), \"\\n\")\n",
    "print(df_normalizado.drop(columns=\"Sample\").max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686b41cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
